<h2>Seeing Beyond Vision</h2>
<p style="font-size:1.1em; color:#555; margin-top:-10px;">
  <i>Deep Stochastic State-Space Models for Occlusion-Robust Control</i>
</p>

<h2>Deep Stochastic State-Space Models (Deep SSSM)</h2>
<p>
  <img src="Pipeline_DeepSSSM.pdf" width="400" align="left" style="margin-right:20px; margin-bottom:10px;" alt="Deep SSSM pipeline diagram" />
  <b>Concept.</b> Deep Stochastic State-Space Models (Deep SSSM) integrate
  control-theoretic reasoning with latent variable learning to achieve
  uncertainty-aware perception and prediction under partial observability.
  Unlike deterministic world models, Deep SSSM explicitly maintains
  probabilistic beliefs over hidden states, enabling robust control
  even when visual information is occluded or corrupted.
</p>

<p>
  <b>Framework.</b> The framework learns a compact latent dynamics model
  from raw image sequences using a variational inference objective:
  it jointly optimizes a transition model, an observation model,
  and a stochastic latent encoderâ€“decoder pair.
  During planning or model-predictive control, the latent dynamics
  are used as a predictive simulator for look-ahead rollouts,
  providing uncertainty-calibrated forecasts of future states and rewards.
</p>

<p>
  <b>Applications.</b> Deep SSSM has been applied to occlusion-robust robotic
  manipulation, adaptive visual tracking, and model-based reinforcement
  learning. By combining Bayesian uncertainty with control-oriented
  rollouts, it bridges the gap between deep representation learning and
  robust adaptive control.
</p>

<p style="font-size:0.9em; color:#666; text-align:center; clear:both;">
  <i>Deep SSSM enables predictive control through latent stochastic dynamics,
  forming the computational core of the Occlusion-Robust Manipulation project.</i>
</p>
