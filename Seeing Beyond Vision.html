<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Tactile Intelligence Loop</title>

<!-- MathJax -->
<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->


	
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
  <!-- Left navigation menu -->
  <td id="layout-menu">
    <div class="menu-category">Thanana</div>
    <div class="menu-item"><a href="index.html">Home</a></div>
    <div class="menu-item"><a href="Biography.html">Biography</a></div>
    <div class="menu-item"><a href="Publication.html">Publication</a></div> 

    <div class="menu-category">Research</div>
    <div class="menu-item"><a href="tactile_intelligence.html" class="current">Tactile&nbsp;Intelligence</a></div>
	<div class="menu-item"><a href="Seeing Beyond Vision.html" class="current">Seeing&nbsp;Beyond Vision</a></div>
    <div class="menu-item"><a href="nonlinear.html">Unstable&nbsp;Systems</a></div>
    <div class="menu-item"><a href="nonholonomic.html">Holonomic</a></div>
    <div class="menu-item"><a href="Cyber.html">Data-driven&nbsp;Control</a></div>
    <div class="menu-item"><a href="large-scale.html">Large-scale&nbsp;System</a></div>

    <div class="menu-category">Teaching</div>
    <div class="menu-item"><a href="reinforcement_learning.html">Reinforcement&nbsp;Learning</a></div>

    <div class="menu-category">Software</div>
    <div class="menu-item"><a href="robotics.html">Robotics</a></div>
    <div class="menu-item"><a href="image.html">Image&nbsp;Processing</a></div>
</td>
<td id="layout-content">
<div id="toptitle">









<h1>Seeing Beyond Vision</h1>
<p style="font-size:1.1em; color:#555; margin-top:-10px;">
  
</p>

<h3>Deep Stochastic State-Space Models (Deep SSSM)</h3>



<img src="DeepSSSM1.pdf" width="400" align="left"
     style="margin-right:20px; margin-bottom:10px;">

	

<div style="margin-top:-10px; margin-bottom:4px;">

	<hr style="border:0; border-top:1px solid #ddd; margin:10px 0 14px 0;">

	


<hr style="border:0; border-top:1px solid #ddd; margin:16px 0 20px 0;">

<h3 style="margin-bottom:6px;">Understanding Deep SSSM</h3>

<p>
  <b>What is Deep SSSM?</b><br>
  Deep Stochastic State-Space Models (Deep SSSM) are a new class of
  uncertainty-aware world models that combine <em>probabilistic latent dynamics</em>
  with <em>control-theoretic planning</em>.
  They allow robots to reason about hidden states — what cannot be directly seen —
  and to predict how actions influence the environment under uncertainty.
</p>

<p>
  <b>What is the key challenge?</b><br>
  In real robotic environments, visual observations are often <em>occluded,
  noisy, or incomplete</em>. Traditional deterministic world models collapse
  under these conditions because they overfit to single “best guesses” of the
  state. The key challenge is to maintain calibrated uncertainty and still
  make reliable control decisions when the world is only partially visible.
</p>

<p>
  <b>How does Deep SSSM address this?</b><br>
  By learning a <em>stochastic latent state</em> that represents uncertainty
  explicitly. The model predicts distributions over future trajectories rather
  than single outcomes. This allows the control layer to plan and adapt based
  on uncertainty, yielding more stable and robust policies during occlusion or
  unseen perturbations.
</p>

<p>
  <b>What is the impact?</b><br>
  Deep SSSM enables <em>occlusion-robust visual control</em> — a step toward
  perception-driven robots that can act safely even when visual data is
  incomplete. The approach provides a foundation for uncertainty-aware
  decision-making in robotic manipulation, visual servoing, and human–robot
  collaboration.
</p>

<p style="font-size:0.85em; color:#777; text-align:center;">
  <i>In short: Deep SSSM teaches robots to “see beyond vision” — reasoning about
  what’s hidden, uncertain, and dynamic.</i>
</p>


<p style="font-size:0.85em; color:#777; text-align:center;">
  <i>Deep SSSM enables predictive control through latent stochastic dynamics,
  forming the computational core of occlusion-robust manipulation.</i>
</p>

