<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Publication</title>
<!-- MathJax -->
	<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
	});
	</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Thanana</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="Biography.html">Biography</a></div>
<div class="menu-item"><a href="Publication.html">Publication</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="nonlinear.html">Unstable&nbsp;systems</a></div>
<div class="menu-item"><a href="nonholonomic.html">Holonomic</a></div>
<div class="menu-item"><a href="Cyber.html">Data-driven control</a></div>
<div class="menu-item"><a href="large-scale.html">Large-scale system</a></div>
<div class="menu-category">Teaching</div>
	<div class="menu-item"><a href="reinforcement_learning.html">Reinforcement Learning</a></div>
<div class="menu-category">Software</div>
<div class="menu-item"><a href="robotics.html">Robotics</a></div>
<div class="menu-item"><a href="image.html">Image&nbsp;Processing</a></div>
</td>
<td id="layout-content">
<div id="toptitle">

<h1>Reinforcement Learning</h1>

<div style="text-align:center; margin: 20px 0;">
  <svg width="780" height="320" viewBox="0 0 780 320" xmlns="http://www.w3.org/2000/svg" style="max-width:100%; height:auto;">
    <defs>
      <marker id="arrow" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto">
        <polygon points="0 0, 10 3.5, 0 7" fill="#0b2740"/>
      </marker>
      <linearGradient id="envGrad" x1="0" y1="0" x2="1" y2="1">
        <stop offset="0%" stop-color="#e5e7eb"/>
        <stop offset="100%" stop-color="#cbd5e1"/>
      </linearGradient>
    </defs>

    <!-- Agent panel (robot + policy/value nets) -->
    <rect x="40" y="50" width="240" height="220" rx="14" fill="#0ea5e9" stroke="#0b2740" stroke-width="2"/>
    <text x="160" y="75" text-anchor="middle" font-family="sans-serif" font-size="14" fill="#ffffff">Agent (Robot Controller)</text>

    <!-- Simple robot arm icon -->
    <g transform="translate(70,95)">
      <!-- base -->
      <rect x="0" y="70" width="36" height="24" rx="4" fill="#0b2740"/>
      <!-- shoulder joint -->
      <circle cx="18" cy="70" r="10" fill="#0b2740"/>
      <!-- upper arm -->
      <rect x="14" y="30" width="8" height="40" fill="#0b2740"/>
      <!-- elbow joint -->
      <circle cx="18" cy="30" r="8" fill="#0b2740"/>
      <!-- forearm -->
      <rect x="16" y="5" width="6" height="25" fill="#0b2740"/>
      <!-- gripper -->
      <path d="M14,0 l-10,-10 l6,0 l8,8 l8,-8 l6,0 l-10,10 z" fill="#0b2740"/>
    </g>

    <!-- policy/value mini-cards -->
    <rect x="120" y="110" width="140" height="50" rx="8" fill="#0284c7" stroke="#0b2740" stroke-width="1.5"/>
    <text x="190" y="140" text-anchor="middle" font-family="sans-serif" font-size="13" fill="#e5f3ff">π(a|s) — Policy Network</text>

    <rect x="120" y="170" width="140" height="50" rx="8" fill="#0284c7" stroke="#0b2740" stroke-width="1.5"/>
    <text x="190" y="200" text-anchor="middle" font-family="sans-serif" font-size="13" fill="#e5f3ff">V(s) / Q(s,a) — Value</text>

    <!-- Environment panel -->
    <rect x="460" y="50" width="280" height="220" rx="14" fill="url(#envGrad)" stroke="#0b2740" stroke-width="2"/>
    <text x="600" y="75" text-anchor="middle" font-family="sans-serif" font-size="14" fill="#1f2937">Environment</text>

    <!-- Table / surface -->
    <rect x="490" y="210" width="220" height="16" rx="3" fill="#94a3b8"/>
    <!-- Object (box) -->
    <rect x="560" y="170" width="50" height="40" rx="4" fill="#f59e0b" stroke="#0b2740" stroke-width="1.5"/>
    <!-- Target area -->
    <rect x="510" y="170" width="32" height="32" rx="6" fill="none" stroke="#16a34a" stroke-dasharray="4 4" stroke-width="2"/>
    <text x="526" y="166" text-anchor="middle" font-family="sans-serif" font-size="11" fill="#166534">goal</text>

    <!-- Camera sensor -->
    <rect x="680" y="110" width="36" height="22" rx="4" fill="#1f2937"/>
    <circle cx="698" cy="121" r="5" fill="#60a5fa"/>
    <text x="698" y="106" text-anchor="middle" font-family="sans-serif" font-size="11" fill="#374151">sensor</text>

    <!-- Arrows: action -> env -->
    <line x1="280" y1="140" x2="460" y2="140" stroke="#0b2740" stroke-width="2" marker-end="url(#arrow)"/>
    <text x="370" y="128" text-anchor="middle" font-family="sans-serif" font-size="13" fill="#0b2740">action (grasp/pose/force)</text>

    <!-- Arrows: observation/reward -> agent -->
    <line x1="460" y1="185" x2="280" y2="185" stroke="#0b2740" stroke-width="2" marker-end="url(#arrow)"/>
    <text x="370" y="203" text-anchor="middle" font-family="sans-serif" font-size="13" fill="#0b2740">observation s′ (image/pose), reward r</text>

    <!-- Reward star near goal -->
    <g transform="translate(520,150)">
      <polygon points="12,0 15,8 24,9 17,15 19,24 12,19 5,24 7,15 0,9 9,8" fill="#22c55e" stroke="#14532d" stroke-width="1"/>
    </g>

    <!-- Legend (small) -->
    <rect x="300" y="250" width="180" height="52" rx="8" fill="#f8fafc" stroke="#cbd5e1"/>
    <circle cx="315" cy="268" r="4" fill="#0b2740"/><text x="330" y="272" font-family="sans-serif" font-size="12" fill="#1f2937">signal flow</text>
    <rect x="308" y="279" width="14" height="8" fill="#0ea5e9" stroke="#0b2740" stroke-width="1"/><text x="330" y="286" font-family="sans-serif" font-size="12" fill="#1f2937">agent modules</text>
  </svg>
</div>

	

<h2>Course description</h2>
<ul>
  <li>This course covers the theory and practice of <strong>Reinforcement Learning (RL)</strong>, focusing on sequential decision-making under uncertainty. Topics include Markov Decision Processes (MDPs), value functions, Bellman operators, dynamic programming, Monte Carlo methods, temporal-difference learning, function approximation, deep RL, policy gradients, actor–critic algorithms, exploration strategies, and safe/model-based RL for robotics and control applications.</li>
</ul>

<h2>Course syllabus</h2>

<table border="1" cellspacing="0" cellpadding="6" style="border-collapse:collapse; width:100%;">
  <thead style="background:#f2f2f2;">
    <tr>
      <th style="width:15%; text-align:left;">Week</th>
      <th style="text-align:left;">Topics</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1–2</td>
      <td>Introduction, MDPs, value functions, Bellman equations, dynamic programming (policy iteration, value iteration)</td>
    </tr>
    <tr>
      <td>3–4</td>
      <td>Monte Carlo methods, temporal-difference learning (TD(0), SARSA, Q-learning), n-step methods, TD(λ)</td>
    </tr>
    <tr>
      <td>5–7</td>
      <td>Function approximation (linear &amp; deep), DQN (target networks, replay buffers)</td>
    </tr>
    <tr>
      <td>8</td>
      <td><strong>Midterm exam</strong> + review</td>
    </tr>
    <tr>
      <td>9–12</td>
      <td>Policy gradients, REINFORCE, actor–critic (A2C/A3C, PPO, TRPO), model-based RL (planning, Dyna, MPC, uncertainty-aware control)</td>
    </tr>
    <tr>
      <td>13–15</td>
      <td>Exploration strategies, reward shaping, safety constraints, off-policy evaluation, multi-agent RL, project presentations</td>
    </tr>
  </tbody>
</table>

<h2>Course objectives</h2>
<ul>
  <li>Equip students with the ability to model, analyze, and implement RL algorithms</li>
  <li>Develop theoretical understanding of convergence, stability, and bias–variance trade-offs</li>
  <li>Enable application of RL methods in simulation and control contexts</li>
  <li>Provide hands-on experience implementing classic and modern RL algorithms</li>
</ul>

<h2>Learning outcomes</h2>
<ul>
  <li>Formulate control and decision problems as MDPs/POMDPs</li>
  <li>Derive and implement dynamic programming, Monte Carlo, and temporal-difference algorithms</li>
  <li>Analyze convergence properties and stability under function approximation</li>
  <li>Design and implement policy gradient and actor–critic algorithms</li>
  <li>Integrate model-based and safe RL components (e.g., MPC) for real-world tasks</li>
</ul>

<h2>Textbooks</h2>
<ul>
  <li><em>Sutton & Barto</em>, <strong>Reinforcement Learning: An Introduction</strong> (2nd ed.)</li>
  <li><em>Puterman</em>, <strong>Markov Decision Processes</strong></li>
  <li><em>Bertsekas</em>, <strong>Dynamic Programming and Optimal Control</strong></li>
  <li>Selected papers and online resources (e.g., Gymnasium, Stable Baselines3, OpenAI Spinning Up)</li>
</ul>

<h2>Midterm exam</h2>
<ul>
  <li><strong>Week 8</strong> — in class, closed book. One A4 cheat sheet allowed.</li>
  <li>Coverage: Topics from Weeks 1–7 (MDPs, Bellman equations, DP, MC, TD, Function Approximation, DQN)</li>
</ul>

<h2>Projects</h2>
<ul>
  <li><strong>Course Project</strong>: Students select a domain (preferably robotics or control), implement one or more RL algorithms, and write a 6–8 page report including methodology, experiments, and analysis.</li>
  <li>Project Proposal: Week 11</li>
  <li>Checkpoint: Week 14</li>
  <li>Final Presentation: Week 15</li>
  <li>Topics may include safe RL, model-based RL, multi-agent RL, exploration strategies, or novel applications.</li>
</ul>

	
<div id="footer">
<div id="footer-text">
Page generated 2019-07-24 18:56:59 CST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>










