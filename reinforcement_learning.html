<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Publication</title>
<!-- MathJax -->
	<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
	});
	</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Thanana</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="Biography.html">Biography</a></div>
<div class="menu-item"><a href="Publication.html">Publication</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="nonlinear.html">Unstable&nbsp;systems</a></div>
<div class="menu-item"><a href="nonholonomic.html">Holonomic</a></div>
<div class="menu-item"><a href="Cyber.html">Data-driven control</a></div>
<div class="menu-item"><a href="large-scale.html">Large-scale system</a></div>
<div class="menu-category">Teaching</div>
	<div class="menu-item"><a href="reinforcement_learning.html">Reinforcement Learning</a></div>
<div class="menu-category">Software</div>
<div class="menu-item"><a href="robotics.html">Robotics</a></div>
<div class="menu-item"><a href="image.html">Image&nbsp;Processing</a></div>
</td>
<td id="layout-content">
<div id="toptitle">

<h1>Reinforcement Learning</h1>

<div style="text-align:center; margin: 20px 0;">
  <svg width="450" height="270" viewBox="0 0 850 500" xmlns="http://www.w3.org/2000/svg" style="max-width:100%; height:auto;">

    <defs>
      <marker id="arrow" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto">
        <polygon points="0 0, 10 3.5, 0 7" fill="#0b2740"/>
      </marker>
      <linearGradient id="coneGrad" x1="0" y1="0" x2="1" y2="0">
        <stop offset="0%" stop-color="#fff3c4" stop-opacity="0.8"/>
        <stop offset="100%" stop-color="#fff3c4" stop-opacity="0.05"/>
      </linearGradient>
    </defs>

    <!-- Drone Symbol -->
    <g id="drone">
      <rect x="-18" y="-10" width="36" height="20" rx="6" fill="#0ea5e9" stroke="#0b2740" stroke-width="2"/>
      <rect x="-10" y="-22" width="20" height="12" rx="3" fill="#0ea5e9" stroke="#0b2740" stroke-width="2"/>
      <circle cx="-4" cy="-16" r="2.7" fill="white"/>
      <circle cx="4" cy="-16" r="2.7" fill="white"/>
      <line x1="-26" y1="-10" x2="26" y2="-10" stroke="#0b2740" stroke-width="2"/>
      <line x1="-34" y1="-16" x2="-18" y2="-16" stroke="#0b2740" stroke-width="3"/>
      <line x1="18" y1="-16" x2="34" y2="-16" stroke="#0b2740" stroke-width="3"/>
      <line x1="-12" y1="10" x2="-12" y2="16" stroke="#0b2740" stroke-width="2"/>
      <line x1="12" y1="10" x2="12" y2="16" stroke="#0b2740" stroke-width="2"/>
      <line x1="-14" y1="16" x2="14" y2="16" stroke="#0b2740" stroke-width="2"/>
    </g>

    <!-- Drones (stacked) with cones -->
    <g transform="translate(120,100)">
      <use href="#drone"/>
      <polygon points="0,0 280,20 280,-20" fill="url(#coneGrad)" stroke="#fbbf24" stroke-width="1.5" opacity="0.9"/>
    </g>
    <g transform="translate(120,200)">
      <use href="#drone"/>
      <polygon points="0,0 280,20 280,-20" fill="url(#coneGrad)" stroke="#fbbf24" stroke-width="1.5" opacity="0.9"/>
    </g>
    <g transform="translate(120,300)">
      <use href="#drone"/>
      <polygon points="0,0 280,20 280,-20" fill="url(#coneGrad)" stroke="#fbbf24" stroke-width="1.5" opacity="0.9"/>
    </g>
    <g transform="translate(120,400)">
      <use href="#drone"/>
      <polygon points="0,0 280,20 280,-20" fill="url(#coneGrad)" stroke="#fbbf24" stroke-width="1.5" opacity="0.9"/>
    </g>

    <!-- Communication lines -->
    <line x1="120" y1="100" x2="120" y2="200" stroke="#0b2740" stroke-dasharray="6,4" stroke-width="1.5"/>
    <line x1="120" y1="200" x2="120" y2="300" stroke="#0b2740" stroke-dasharray="6,4" stroke-width="1.5"/>
    <line x1="120" y1="300" x2="120" y2="400" stroke="#0b2740" stroke-dasharray="6,4" stroke-width="1.5"/>

    <!-- Environment -->
    <path d="M600 250 
             q20 -80 60 -50 
             q40 -30 80 10 
             q40 0 40 80 
             q0 60 -40 70 
             q-20 50 -80 30 
             q-50 20 -60 -40 
             q-40 -20 -40 -70 
             q0 -40 40 -30 Z"
          fill="#f1f5f9" stroke="#0b2740" stroke-width="2"/>

    <!-- Goal, obstacle, uncertainty -->
    <circle cx="675" cy="270" r="12" fill="#22c55e" stroke="#14532d" stroke-width="2"/>
    <text x="675" y="300" text-anchor="middle" font-size="11" fill="#14532d" font-family="sans-serif">goal</text>
    <rect x="720" y="250" width="30" height="30" fill="#f59e0b" stroke="#92400e" stroke-width="2"/>
    <text x="760" y="220" text-anchor="middle" font-family="sans-serif" font-size="32" fill="#0b2740" font-weight="bold">?</text>

	  <!-- Question mark for uncertainty -->

    <text x="700" y="200" text-anchor="middle" font-family="sans-serif" font-size="14" fill="#1f2937">Environment / World</text> 

    <!-- Action arrows -->
    <line x1="200" y1="100" x2="580" y2="250" stroke="#0b2740" stroke-width="2" marker-end="url(#arrow)"/>
    <line x1="200" y1="200" x2="580" y2="250" stroke="#0b2740" stroke-width="2" marker-end="url(#arrow)"/>
    <line x1="200" y1="300" x2="580" y2="250" stroke="#0b2740" stroke-width="2" marker-end="url(#arrow)"/>
    <line x1="200" y1="400" x2="580" y2="250" stroke="#0b2740" stroke-width="2" marker-end="url(#arrow)"/>

    <!-- Observations back -->
    <line x1="580" y1="300" x2="200" y2="250" stroke="#0b2740" stroke-width="2" marker-end="url(#arrow)"/>

  </svg>
</div>
	






	<h2>Prerequisites</h2>
<ul>
	Supervised & Unsupervised Learning, Neural Networks. Good knowledge of linear algebra and probability.

</ul>

<h2>Course description</h2>
<ul>
  <li>This course covers the theory and practice of <strong>Reinforcement Learning (RL)</strong>, focusing on sequential decision-making under uncertainty. Topics include Markov Decision Processes (MDPs), value functions, Bellman operators, dynamic programming, Monte Carlo methods, temporal-difference learning, function approximation, deep RL, policy gradients, actor–critic algorithms, exploration strategies, and safe/model-based RL for robotics and control applications.</li>
</ul>


	
<h2>Course objectives</h2>
<ul>
  <li>Equip students with the ability to model, analyze, and implement RL algorithms</li>
  <li>Develop theoretical understanding of convergence, stability, and bias–variance trade-offs</li>
  <li>Enable application of RL methods in simulation and control contexts</li>
  <li>Provide hands-on experience implementing classic and modern RL algorithms</li>
</ul>



<h2>Learning outcomes</h2>
<ul>
  <li>Formulate control and decision problems as MDPs/POMDPs</li>
  <li>Derive and implement dynamic programming, Monte Carlo, and temporal-difference algorithms</li>
  <li>Analyze convergence properties and stability under function approximation</li>
  <li>Design and implement policy gradient and actor–critic algorithms</li>
  <li>Integrate model-based and safe RL components (e.g., MPC) for real-world tasks</li>
</ul>




<h2>Course Schedule (15 Weeks) and Materials</h2>
<table border="1" cellspacing="0" cellpadding="5">
  <tr>
    <th>Week</th>
    <th>Topic</th>
    <th>Slides</th>
  </tr>
  <tr>
    <td>1</td>
    <td>Introduction to RL, Agent–Environment Interface</td>
    <td><a href="slides/lecture01_intro.pdf">Lecture 1</a></td>
  </tr>
  <tr>
    <td>2</td>
    <td>Markov Decision Processes (MDPs)</td>
    <td><a href="slides/lecture02_mdps.pdf">Lecture 2</a></td>
  </tr>
  <tr>
    <td>3</td>
    <td>Value Functions, Bellman Equations</td>
    <td><a href="slides/lecture03_value_functions.pdf">Lecture 3</a></td>
  </tr>
  <tr>
    <td>4</td>
    <td>Dynamic Programming</td>
    <td><a href="slides/lecture04_dp.pdf">Lecture 4</a></td>
  </tr>
  <tr>
    <td>5</td>
    <td>Monte Carlo Methods</td>
    <td><a href="slides/lecture05_mc.pdf">Lecture 5</a></td>
  </tr>
  <tr>
    <td>6</td>
    <td>Temporal Difference Learning</td>
    <td><a href="slides/lecture06_td.pdf">Lecture 6</a></td>
  </tr>
  <tr>
    <td>7</td>
    <td>Midterm Review & Exam</td>
    <td>—</td>
  </tr>
  <tr>
    <td>8</td>
    <td>Policy Gradient Methods</td>
    <td><a href="slides/lecture07_policy_gradients.pdf">Lecture 7</a></td>
  </tr>
  <tr>
    <td>9</td>
    <td>Actor–Critic Methods</td>
    <td><a href="slides/lecture08_actor_critic.pdf">Lecture 8</a></td>
  </tr>
  <tr>
    <td>10</td>
    <td>Deep Reinforcement Learning (I)</td>
    <td><a href="slides/lecture09_deep_rl1.pdf">Lecture 9</a></td>
  </tr>
  <tr>
    <td>11</td>
    <td>Deep Reinforcement Learning (II)</td>
    <td><a href="slides/lecture10_deep_rl2.pdf">Lecture 10</a></td>
  </tr>
  <tr>
    <td>12</td>
    <td>Applications in Robotics, Games, NLP</td>
    <td><a href="slides/lecture11_applications.pdf">Lecture 11</a></td>
  </tr>
  <tr>
    <td>13</td>
    <td>Project Work & Advanced Topics (Safe RL, MARL)</td>
    <td><a href="slides/lecture12_safe_marl.pdf">Lecture 12</a></td>
  </tr>
  <tr>
    <td>14</td>
    <td>Presentations & Integration</td>
    <td><a href="slides/lecture13_presentations.pdf">Lecture 13</a></td>
  </tr>
  <tr>
    <td>15</td>
    <td>Final Review & Exam</td>
    <td><a href="slides/lecture14_wrapup.pdf">Lecture 14</a></td>
  </tr>
</table>







<h2>Textbooks</h2>
<ul>
  <li><em>Sutton & Barto</em>, <strong>Reinforcement Learning: An Introduction</strong> (2nd ed.)</li>
  <li><em>Puterman</em>, <strong>Markov Decision Processes</strong></li>
  <li><em>Bertsekas</em>, <strong>Dynamic Programming and Optimal Control</strong></li>
  <li>Selected papers and online resources (e.g., Gymnasium, Stable Baselines3, OpenAI Spinning Up)</li>
</ul>

<h2>Midterm exam</h2>
<ul>
  <li><strong>Week 8</strong> — in class, closed book. One A4 cheat sheet allowed.</li>
  <li>Coverage: Topics from Weeks 1–7 (MDPs, Bellman equations, DP, MC, TD, Function Approximation, DQN)</li>
</ul>

<h2>Projects</h2>
<ul>
  <li><strong>Course Project</strong>: Students select a domain (preferably robotics or control), implement one or more RL algorithms, and write a 6–8 page report including methodology, experiments, and analysis.</li>
  <li>Project Proposal: Week 11</li>
  <li>Checkpoint: Week 14</li>
  <li>Final Presentation: Week 15</li>
  <li>Topics may include safe RL, model-based RL, multi-agent RL, exploration strategies, or novel applications.</li>
</ul>

	
<div id="footer">
<div id="footer-text">
Page generated 2019-07-24 18:56:59 CST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>










